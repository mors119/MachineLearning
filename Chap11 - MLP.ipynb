{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chap11 - MLP.ipynb","provenance":[{"file_id":"1_qR-sMLkrqqxMPWV142hpZ4KUn-xVxqK","timestamp":1618796760106}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["# **MLP(Multi-Layer Perceptron) 구현하기**"],"metadata":{"id":"3eoY2gbxgU_Q"}},{"cell_type":"markdown","source":["**데이터 로드 및 전처리**\n","\n","먼저, 가장 널리 사용되는 딥러닝 프레임워크(framework) 중 하나인 텐서플로(TensorFlow)를 임포트하고, \n","\n","라이브러리로부터 MNIST데이터를 로드하고 전처리를 수행한다.\n","\n","클래스 레이블을 원핫벡터(one-hot vector)로 변환하는 과정은 사용하고자 하는 프레임워크와 모델의 요구에 맞추어 선택적으로 거치는 과정이다."],"metadata":{"id":"gFJfGaSokkEr"}},{"cell_type":"code","metadata":{"id":"2paCC6mHhy9M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653110964759,"user_tz":-540,"elapsed":858,"user":{"displayName":"이관용","userId":"09728382867624819840"}},"outputId":"60a82e6a-7073-40d4-e651-e42889e288fd"},"source":["import os\n","\n","\n","# Import tensorflow packages\n","import tensorflow as tf\n","import numpy as np\n","\n","# MNIST 데이터 로드 및 전처리\n","mnist = tf.keras.datasets.mnist\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0    # 데이터 정규화\n","\n","print(np.shape(x_test))\n","print(np.shape(y_test))\n","print(np.unique(y_test), '\\n') # 클래스 레이블 확인\n","\n","# 클래스 레이블 one-hot vector 변환\n","y_train_1hot = tf.one_hot(y_train, 10)\n","y_test_1hot = tf.one_hot(y_test, 10)\n","print('one-hot vector 변환 후 테스트 데이터 클래스 레이블 차원 수 : ', np.shape(y_test_1hot))\n","print('클래스 ', y_test[0], '의 one-hot vector : ', y_test_1hot[0,:])"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["(10000, 28, 28)\n","(10000,)\n","[0 1 2 3 4 5 6 7 8 9] \n","\n","one-hot vector 변환 후 테스트 데이터 클래스 레이블 차원 수 :  (10000, 10)\n","클래스  7 의 one-hot vector :  tf.Tensor([0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], shape=(10,), dtype=float32)\n"]}]},{"cell_type":"markdown","source":["**다층 퍼셉트론 모델 생성**\n","\n","모델 선언을 통해 2개의 은닉층으로 이루어진 다층 퍼셉트론(MLP)을 구성한다.\n","\n","분류할 클래스의 수가 3개 이상이고, one-hot vector로 레이블이 주어질 경우 categorical crossentropy를 사용한다."],"metadata":{"id":"nDxfqbAolHtL"}},{"cell_type":"code","metadata":{"id":"M4htiOLzh4VA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653111161989,"user_tz":-540,"elapsed":349,"user":{"displayName":"이관용","userId":"09728382867624819840"}},"outputId":"676f480a-838b-4c63-def6-7e24ddd1249b"},"source":["# 다층 퍼셉트론 모델 생성\n","model = tf.keras.models.Sequential([\n","  tf.keras.layers.Flatten(input_shape = (28, 28)),    # Flatten 28x28 and set input layer with 756 nodes\n","  tf.keras.layers.Dense(128, activation = 'relu'),    # 128개 노드를 가진 첫 번째 은닉층 (relu activation function)\n","  # activations : 'sigmoid','tanh', 'relu'\n","  tf.keras.layers.Dropout(0.2), \n","  tf.keras.layers.BatchNormalization(),\n","  tf.keras.layers.Dense(128, activation = 'relu'),    # 128개 노드를 가진 두 번째 은닉층 (relu activation function)\n","  tf.keras.layers.Dropout(0.2), \n","  tf.keras.layers.BatchNormalization(),\n","  tf.keras.layers.Dense(10, activation = 'softmax')   # 10개(MNIST의 클래스 수) 출력 노드를 가진 출력층 (activation function: 'softmax')\n","])\n","\n","# 생성된 모델의 구조 확인하기\n","model.summary()"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," flatten_1 (Flatten)         (None, 784)               0         \n","                                                                 \n"," dense_3 (Dense)             (None, 128)               100480    \n","                                                                 \n"," dropout_2 (Dropout)         (None, 128)               0         \n","                                                                 \n"," batch_normalization_2 (Batc  (None, 128)              512       \n"," hNormalization)                                                 \n","                                                                 \n"," dense_4 (Dense)             (None, 128)               16512     \n","                                                                 \n"," dropout_3 (Dropout)         (None, 128)               0         \n","                                                                 \n"," batch_normalization_3 (Batc  (None, 128)              512       \n"," hNormalization)                                                 \n","                                                                 \n"," dense_5 (Dense)             (None, 10)                1290      \n","                                                                 \n","=================================================================\n","Total params: 119,306\n","Trainable params: 118,794\n","Non-trainable params: 512\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["**학습 전략 설정**\n","\n","최적화 알고리즘 및 손실함수 등을 설정한다.\n","\n","SGD(Stochastic Gradient Descent), Adam(Adaptive Moment Estimation), RMSprop 등의 최적화 기법들은 각자 장단점이 있으며, \n","\n","풀고자 하는 문제, 신경망 모델과 데이터의 특성에 따라 최적의 효율을 가지는 최적화 기법이 다르므로\n","\n","여러번의 실험을 통해 적절한 최적화 기법을 선택해야 한다. "],"metadata":{"id":"pd45v0ufzFQ0"}},{"cell_type":"code","source":["opt = tf.keras.optimizers.SGD(learning_rate = 0.01, momentum = 0.0)             \n","#opt = tf.keras.optimizers.Adam(learning_rate = 0.001)\n","#opt = tf.keras.optimizers.RMSprop(learning_rate = 0.001, momentum = 0.0)\n","\n","model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])\n","#model.compile(loss = 'mse', optimizer = opt, metrics=['accuracy'])  \n","\n","#model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"],"metadata":{"id":"3xeCiz7vyCys","executionInfo":{"status":"ok","timestamp":1653111381645,"user_tz":-540,"elapsed":1,"user":{"displayName":"이관용","userId":"09728382867624819840"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["**모델 학습 및 성능 측정**\n","\n","앞서 구성한 다층 퍼셉트론 모델을 학습 데이터로 훈련시키고, 테스트 데이터에 대한 성능을 측정한다.\n","\n","fit 함수에서 파라미터로 설정 가능한 'batch_size'는 데이터를 학습할 때 한 번에 몇 개의 데이터로 학습할 것인지를 의미하며,\n","\n","'epoch'는 전체 데이터를 몇 번 반복 학습할 것인지를 나타낸다.\n","\n","여기에서는 time 라이브러리를 통해 학습에 걸린 시간을 측정해 본다.\n","\n","'time.time()'함수는 프로그램이 실행된 이후 현재 시점의 시간을 리턴한다.\n"],"metadata":{"id":"xeJgUlNqnJ1G"}},{"cell_type":"code","metadata":{"id":"222D53WGh66Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653111686512,"user_tz":-540,"elapsed":11373,"user":{"displayName":"이관용","userId":"09728382867624819840"}},"outputId":"07c0031a-ae5a-40ea-dfc2-a9571a4048d4"},"source":["import time\n","\n","# 모델 학습\n","start_time = time.time()\n","model.fit (x_train, y_train_1hot, batch_size = 200, epochs = 5)        # Training with \"categorical crossentropy\"\n","print(\"모델 학습 완료. 학습 시간 : {}\\n\".format(time.time() - start_time))\n","\n","# 테스트 데이터로 학습된 모델의 성능 측정\n","print('테스트 데이터에 대한 분류 성능')\n","test_loss, test_acc = model.evaluate(x_test,  y_test_1hot, batch_size = 100, verbose = 2)"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","300/300 [==============================] - 2s 7ms/step - loss: 0.1524 - accuracy: 0.9545\n","Epoch 2/5\n","300/300 [==============================] - 2s 7ms/step - loss: 0.1484 - accuracy: 0.9561\n","Epoch 3/5\n","300/300 [==============================] - 2s 7ms/step - loss: 0.1456 - accuracy: 0.9567\n","Epoch 4/5\n","300/300 [==============================] - 2s 7ms/step - loss: 0.1446 - accuracy: 0.9570\n","Epoch 5/5\n","300/300 [==============================] - 2s 7ms/step - loss: 0.1374 - accuracy: 0.9583\n","모델 학습 완료. 학습 시간 : 10.834697723388672\n","\n","테스트 데이터에 대한 분류 성능\n","100/100 - 0s - loss: 0.1010 - accuracy: 0.9685 - 234ms/epoch - 2ms/step\n"]}]},{"cell_type":"markdown","source":["위 코드의 실행 결과창을 살펴보면, 총 5번의 epoch 동안 학습하였으며, 각 epoch마다 '300/300'이라는 숫자를 확인할 수 있다.\n","\n","이는 'iteration'을 나타내는 것으로, 한 번에 batch_size만큼 입력하여 전체 학습 데이터를 모두 학습하는 데 걸리는 반복 횟수를 의미한다.\n","\n","즉, 200의 batch_size만큼 300번을 반복해야 총 60,000개의 학습 데이터를 모두 한 번씩 학습할 수 있다는 것이다.\n","\n","이처럼 batch_size를 나누어 한 번에 입력되는 학습 데이터의 개수를 제한하는 것은 메모리와 학습 속도 때문이다.\n","\n","한 번에 모든 데이터를 입력하여 학습하는 경우 메모리 부족 문제로  학습 속도가 느려지거나 불가능하게 된다.\n","\n","이러한 문제는 데이터의 개수가 많을수록, 데이터의 차원이 클수록 더 심해진다.\n","\n"],"metadata":{"id":"NCx6R2Ln20z9"}}]}