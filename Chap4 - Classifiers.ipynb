{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chap4 - Classifiers.ipynb","provenance":[{"file_id":"1so6rOQcEh3u6sNNKYxKniYHswXwCzHnN","timestamp":1616926972730},{"file_id":"1_f_sOoHY5A-kfh0krjg4OIz6Ka9oCm24","timestamp":1616925085070}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **분류기 구현하기**"],"metadata":{"id":"BUJPceXpieXs"}},{"cell_type":"markdown","source":["**구글 드라이브 마운트**\n","\n","먼저, 구글 드라이브에 MNIST_digit159_classify.npz를 업로드하고, 이를 사용하기 위해 구글 드라이브를 마운트한다."],"metadata":{"id":"syeGdT-qisG5"}},{"cell_type":"code","metadata":{"id":"yHdPEt7Wt7ij","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653094431556,"user_tz":-540,"elapsed":23653,"user":{"displayName":"이관용","userId":"09728382867624819840"}},"outputId":"ee2d5831-53f0-49a4-9dd8-54b1e319dbf9"},"source":["# 구글 드라이브 마운트\n","from google.colab import drive\n","drive.mount('/content/gdrive',force_remount=True)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"markdown","source":["**데이터 로드 및 시각화(visualization)**\n","\n","구글 드라이브에 저장된 MNIST 숫자 데이터를 로드하고, 학습 데이터와 테스트 데이터로 할당한 뒤 시각화 한다.\n","\n","분류 문제에서는 데이터(x_train, x_test)와 함께 목표값(ground-truth)인 클래스 라벨(y_train, y_test)도 함께 주어져야 한다.\n","\n","학습 데이터와 테스트 데이터는 각각 300개씩(각 클래스당 100개)의 데이터와 클래스 라벨로 구성되어 있다."],"metadata":{"id":"yrJIM8-4i1PF"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":357},"id":"l9csUVr8ma2P","executionInfo":{"status":"ok","timestamp":1653094438540,"user_tz":-540,"elapsed":1612,"user":{"displayName":"이관용","userId":"09728382867624819840"}},"outputId":"46dc2d3e-8357-4746-af89-a70579f1ac32"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# MNIST 데이터 불러오기 (3개의 클래스(1, 5, 9)로 구성된 28x28 크기의 데이터)\n","# 내 구글 드라이브의 MyColab폴더 안 Data폴더에 업로드된 npz 파일을 로드\n","npzfile = np.load('/content/gdrive/My Drive/MyColab/Data/MNIST_digit159_classify.npz')\n","\n","# 학습 데이터와 테스트 데이터를 라벨과 함께 변수에 할당\n","x_train=npzfile['xtrain'] # 784차원의 학습 데이터\n","y_train=npzfile['ytrain'] # 학습 데이터에 대한 1차원의 클래스 라벨\n","x_test=npzfile['xtest'] # 784차원의 테스트 데이터\n","y_test=npzfile['ytest'] # 테스트 데이터에 대한 1차원의 클래스 라벨\n","\n","# 데이터를 로드한 뒤에는 차원 수를 확인해야 한다.\n","print('x_train 의 dimension', x_train.shape)\n","print('y_train 의 dimension', y_train.shape)\n","print('x_test 의 dimension', x_test.shape)\n","print('y_test 의 dimension', y_test.shape)\n","\n","# 샘플 이미지 시각화\n","X = np.array([np.reshape(t,(28,28)) for t in x_train]) / 255.0\n","plt.imshow(X[99,:,:])"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["x_train 의 dimension (300, 784)\n","y_train 의 dimension (300,)\n","x_test 의 dimension (300, 784)\n","y_test 의 dimension (300,)\n"]},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f6bc8588c10>"]},"metadata":{},"execution_count":3},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["**1. 최소거리 분류기 구현하기**\n","\n","입력 데이터와 각 클래스의 평균 사이의 거리를 계산하여 최소 거리값을 가지는 클래스로 분류하는 최소거리 분류기를 구현해본다."],"metadata":{"id":"-bpFIryyoTEN"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SaKnjGBkoBoU","executionInfo":{"status":"ok","timestamp":1653094442603,"user_tz":-540,"elapsed":285,"user":{"displayName":"이관용","userId":"09728382867624819840"}},"outputId":"a240cea4-065a-4d1c-f862-d06fb5fdc683"},"source":["# 각 클래스에 대한 평균 계산\n","m = np.zeros((3,784)) # 계산된 평균값을 저장할 0값으로 초기화된 3x784 크기의 행렬 생성\n","m[0,:] = np.mean(x_train[0:100,0:784], axis = 0)\n","m[1,:] = np.mean(x_train[100:200,0:784], axis = 0)\n","m[2,:] = np.mean(x_train[200:300,0:784], axis = 0)\n","\n","# 테스트 데이터와 각 클래스의 평균과의 거리 계산하기\n","d1 = np.zeros((3,1))\n","y_out = np.zeros(300)\n","for i in range(300):\n","  t = x_test[i,:]\n","  for k in range(3):\n","    d1[k,0] = np.linalg.norm(t - m[k,:]) # np.linalg.norm 함수는 default가 l2-norm이다.\n","    y_out[i] = np.argmin(d1) + 1  # 테스트 데이터에 대한 예측값 \n","print(y_out, '\\n')\n","\n","# 테스트 데이터에 대한 성능 측정\n","print('Classification error:')\n","print(np.sum(np.abs(y_out - y_test) > 0) / 300, '\\n')\n","print('Classification rate:')\n","print(np.sum(np.abs(y_out - y_test) == 0) / 300,'\\n')"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 2. 1. 2. 2.\n"," 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n"," 2. 2. 2. 3. 2. 2. 2. 2. 2. 2. 3. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n"," 2. 2. 2. 1. 2. 2. 2. 2. 3. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 3. 2. 2. 2.\n"," 2. 2. 2. 2. 2. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n"," 3. 3. 3. 3. 3. 3. 3. 3. 2. 3. 3. 3. 3. 3. 3. 1. 3. 3. 3. 3. 3. 3. 3. 3.\n"," 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n"," 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n"," 3. 3. 3. 3. 3. 3. 3. 3. 3. 2. 3. 1.] \n","\n","Classification error:\n","0.05 \n","\n","Classification rate:\n","0.95 \n","\n"]}]},{"cell_type":"markdown","source":["**2. 사이킷런(scikit-learn)을 사용한 베이즈 분류기 구현하기**\n","\n","베이즈 분류기를 직접 구현할 수도 있지만 파이썬에서는 사이킷런 라이브러리로부터 미리 구현된 모듈을 호출하여 사용할 수 있다.\n","\n","사이킷런이 제공하는 나이브 베이즈 분류기는 여러 가지가 있으며, 여기에서는 가우시안, 베르누이, 다항분포 세 가지만 사용해 본다.\n","\n","가우시안 베이즈 분류기는 데이터가 연속적인 값을 가질 때 많이 사용되며, 베르누이와 다항분포 베이즈 분류기는 주로 텍스트 분류에 많이 사용된다.\n","\n"],"metadata":{"id":"IC7SRNnRsTVJ"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"soHtH22XoiqT","executionInfo":{"status":"ok","timestamp":1653095399166,"user_tz":-540,"elapsed":704,"user":{"displayName":"이관용","userId":"09728382867624819840"}},"outputId":"c633d916-bc08-4ae0-938b-3f0d6b3c72b3"},"source":["# 가우시안 나이브 베이즈\n","from sklearn.naive_bayes import GaussianNB\n","\n","model = GaussianNB() # 분류 모델 생성\n","# 아래와 같이 파라미터로 선험확률을 다르게 주어 모델을 생성할 수 있다.\n","# model = GaussianNB(priors = [10/100, 20/100, 70/100])\n","\n","# model의 파라미터를 가져오고 싶다면 get_params 함수 사용\n","GNB_params = model.get_params().items() # default값으로 설정된 파라미터 확인\n","print(GNB_params, '\\n')\n","\n","# fit 함수를 사용하여 학습 데이터로 모델을 학습\n","model.fit(x_train,y_train) \n","\n","# predict 함수로 학습된 모델에 테스트 데이터를 입력하여 예측값을 얻음\n","y_GNB = model.predict(x_test) \n","print(y_GNB, '\\n')\n","\n","# 테스트 데이터에 대한 성능 측정\n","print('Classification error:')\n","print(np.sum(np.abs(y_GNB - y_test) > 0) / 300, '\\n')\n","print('Classification rate:')\n","print(np.sum(np.abs(y_GNB - y_test) == 0) / 300, '\\n')"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["dict_items([('priors', None), ('var_smoothing', 1e-09)]) \n","\n","[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 2 1 1 1 1 1 3 1 1 1 1\n"," 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1\n"," 1 1 2 1 1 1 1 1 1 1 1 1 3 2 1 1 1 1 1 1 3 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2 2 2 2 2 2 2 2 3 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3\n"," 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2 2 3 2 2 2 2 2 2 2 2 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n"," 3 3 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n"," 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n"," 3 2 3 2] \n","\n","Classification error:\n","0.07333333333333333 \n","\n","Classification rate:\n","0.9266666666666666 \n","\n"]}]},{"cell_type":"markdown","source":["아래 코드를 통해 confusion matrix를 생성하여 각 클래스로 분류된 데이터의 개수를 확인해 볼 수 있다.\n","\n","결과로 표시된 Confusion matrix의 첫 행인 [90, 7, 3]은 첫 번째 클래스를 목표값으로 가지는 100개의 데이터가 첫 번째 클래스로 90개, 두 번째 클래스로 7개, 세 번째 클래스로 3개 분류되었음을 나타낸다."],"metadata":{"id":"9hPmdjq7BijS"}},{"cell_type":"code","source":["# Confusion matrix를 통해 각 클래스로 분류된 데이터의 개수 확인\n","from sklearn.metrics import confusion_matrix  \n","confusion_matrix(y_test, y_GNB)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kYUTxpjPBMN9","executionInfo":{"status":"ok","timestamp":1646730267459,"user_tz":-540,"elapsed":320,"user":{"displayName":"Changhun Hyun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17016822345634441329"}},"outputId":"a825eafd-eff7-405e-ed88-0063acd6d3bd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[90,  7,  3],\n","       [ 0, 94,  6],\n","       [ 0,  6, 94]])"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["# 베르누이 나이브 베이즈\n","from sklearn.naive_bayes import BernoulliNB\n","\n","model2 = BernoulliNB()\n","model2.fit(x_train, y_train) \n","\n","y_BNB = model2.predict(x_test) \n","\n","# 테스트 데이터에 대한 성능 측정\n","print('Classification error:')\n","print(np.sum(np.abs(y_BNB - y_test) > 0) / 300, '\\n')\n","print('Classification rate:')\n","print(np.sum(np.abs(y_BNB - y_test) == 0) / 300, '\\n')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rUrGOCs_qDGw","executionInfo":{"status":"ok","timestamp":1653095595513,"user_tz":-540,"elapsed":309,"user":{"displayName":"이관용","userId":"09728382867624819840"}},"outputId":"319fbca8-f2ae-4e99-c652-1b382cc82b3e"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Classification error:\n","0.04666666666666667 \n","\n","Classification rate:\n","0.9533333333333334 \n","\n"]}]},{"cell_type":"code","source":["# 다항분포 나이브 베이즈\n","from sklearn.naive_bayes import MultinomialNB\n","\n","model3 = MultinomialNB()\n","model3.fit(x_train, y_train) \n","\n","y_MNB = model3.predict(x_test) \n","\n","# 테스트 데이터에 대한 성능 측정\n","print('Classification error:')\n","print(np.sum(np.abs(y_MNB - y_test) > 0) / 300, '\\n')\n","print('Classification rate:')\n","print(np.sum(np.abs(y_MNB - y_test) == 0) / 300, '\\n')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zOtq9JOZZ-yG","executionInfo":{"status":"ok","timestamp":1653095621532,"user_tz":-540,"elapsed":306,"user":{"displayName":"이관용","userId":"09728382867624819840"}},"outputId":"7865c765-fd62-4d89-df53-379dc3e85793"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Classification error:\n","0.05 \n","\n","Classification rate:\n","0.95 \n","\n"]}]},{"cell_type":"markdown","source":["**3. K-최근접이웃 분류기 구현하기**\n","\n","사이킷런으로부터 K-NN 분류기를 호출하여 MNIST 데이터를 분류해 본다.\n","\n","먼저, K가 10인 경우의 K-NN 분류기를 구현해 보고, K값에 따른 성능 변화를 살펴본다."],"metadata":{"id":"C4z9Vxific2e"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JzokLY6lo4UE","executionInfo":{"status":"ok","timestamp":1646113699583,"user_tz":-540,"elapsed":305,"user":{"displayName":"Changhun Hyun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17016822345634441329"}},"outputId":"25099418-45f1-40cf-dce6-d2774e4535c8"},"source":["from scipy.spatial import distance\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","K = 10      # neighbor 개수 정하기\n","\n","# 모델 생성 및 학습\n","model = KNeighborsClassifier(n_neighbors = K)\n","model.fit(x_train,y_train)\n","\n","# 테스트 데이터에 대한 예측값 획득\n","y_KNN = model.predict(x_test)\n","print(y_KNN, '\\n')\n","\n","# 테스트 데이터에 대한 성능 측정\n","print('Classification error:')\n","print(np.sum(np.abs(y_KNN - y_test) > 0) / 300, '\\n')\n","print('Classification rate:')\n","print(np.sum(np.abs(y_KNN - y_test) == 0) / 300, '\\n')\n","\n","# 분류율은 score 함수로도 구할 수 있다.\n","acc = model.score(x_test, y_test)\n","print(acc)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 3\n"," 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 1 2 2 2 2 3 2 2 2 2 2 2 3 2\n"," 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n"," 3 3 2 3 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n"," 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n"," 3 3 3 1] \n","\n","Classification error:\n","0.03333333333333333 \n","\n","Classification rate:\n","0.9666666666666667 \n","\n","0.9666666666666667\n"]}]},{"cell_type":"markdown","source":["For문을 활용하여 K-NN 분류기의 K값에 따른 분류 성능 변화를 관찰한다. "],"metadata":{"id":"TAbEl1S5Ciya"}},{"cell_type":"code","source":["# neighbor의 개수를 5, 10, 15, 20, 25, 30 으로 설정한 경우의 분류율 비교하기\n","K_values = [5, 10, 15, 20, 25, 30]\n","for k in K_values:\n","    model = KNeighborsClassifier(n_neighbors = k)\n","    model.fit(x_train,y_train)\n","    y_KNN = model.predict(x_test)\n","    print('Classification rate with k =', k)\n","    print(np.sum(np.abs(y_KNN - y_test) == 0) / 300, '\\n')\n","    "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sU9nrMm6vPD9","executionInfo":{"status":"ok","timestamp":1646113701612,"user_tz":-540,"elapsed":279,"user":{"displayName":"Changhun Hyun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17016822345634441329"}},"outputId":"b2c11352-e087-405f-b9d5-7002260d7df4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Classification rate with k = 5\n","0.97 \n","\n","Classification rate with k = 10\n","0.9666666666666667 \n","\n","Classification rate with k = 15\n","0.9666666666666667 \n","\n","Classification rate with k = 20\n","0.9533333333333334 \n","\n","Classification rate with k = 25\n","0.9466666666666667 \n","\n","Classification rate with k = 30\n","0.9366666666666666 \n","\n"]}]}]}